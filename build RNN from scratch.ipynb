{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import itertools\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('reddit-comments-2015-08.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = 'UNKNOWN_TOKEN'\n",
    "sentence_start_token = 'SENTENCE_START'\n",
    "sentence_end_token = 'SENTENCE_END'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [nltk.word_tokenize(sentence_start_token + ' ' + data['body'][i] + ' ' + sentence_end_token) for i in range(data.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = nltk.FreqDist(itertools.chain(*tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = word_freq.most_common(vocabulary_size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = dict([(w,i) for i, w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(tokens):\n",
    "    tokens[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokens])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, word_dim, hidden_dim = 100, bptt_truncate = 4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim),(word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim),(hidden_dim, hidden_dim))\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hidden states in s because we need them later\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "#         s[-1] = np.zeros(self.hidden_dim)\n",
    "        # The outputs at each time step. Again, we save them for later\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        # For each time step...\n",
    "        for t in range(T):\n",
    "            s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "            \n",
    "        return [o,s]\n",
    "    \n",
    "    def predict(self,x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis = 1)\n",
    "    \n",
    "    # calculating the Loss\n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "        # For each sentence:\n",
    "        for i in range(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            correct_word_predictions = o[np.arange(len(y[i])),y[i]]\n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    "    \n",
    "    def calculate_loss(self, x, y):\n",
    "        N = np.sum(len(y_i) for y_i in y)\n",
    "        return calculate_total_loss(x,y)/N\n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # For each output backwards\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T) # entropy loss derivative\n",
    "            # initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "            for bptt_step in range(max(0, t - self.bptt_truncate), t+1)[::-1]:\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])\n",
    "                dLdU[:,x[bptt_step]] +=  delta_t * x[t] \n",
    "                # for some reason the original script does not include *x[t],\n",
    "                # which i think is wrong\n",
    "                # update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step -1] ** 2)\n",
    "                # there should be multplying another s[bptt_step - 1]\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        # Calculate the gradients\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        # update weights\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW\n",
    "        \n",
    "    def train_with_sgd(model, X_train, y_train, learning_rate = 0.005, nepoch = 5, evaluate_loss_after=5):\n",
    "        losses = []\n",
    "        num_examples_seen = 0\n",
    "        for epoch in range(nepoch):\n",
    "            # Optionally evaluate the loss\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                loss = model.calculate_loss(X_train, y_train)\n",
    "                losses.append((num_examples_seen, loss))\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "                # Adjust the learning rate if loss increases\n",
    "                if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                    learning_rate = learning_rate * 0.5  \n",
    "                    print(\"Setting learning rate to %f\" % learning_rate)\n",
    "                sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23 s ± 45.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNN(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
